##讲稿

我读的这篇论文的题目叫做测量信息的传递

首先来看一下这篇论文在做一件什么事情。

在统计学中，我们使用概率来表征事件的可能性，使用条件概率，联合概率等来衡量不同随机变量之间的关系。在信息论中，我们使用香农熵来衡量一个事件或者系统所包含的信息量得多少，或者是不确定度的大小，使用互信息来在一定程度上表征两个系统之间的不确定度或者信息量的关系

互信息是对称的，它不包含任何方向信息。也提出过使用包含了延迟因子的互信息来让它携带一定的方向信息，但是这个并不能揭示任何底层原理，而更类似于一种技术手段。

于是这里提出了一个新的量--转移熵。转移熵可以用来定量的描述两个系统之间的因果性，或者说系统之间的信息流向和大小。

这个量的目标就是对互信息做一些改进，提升他在衡量信息转移上的表现。主要针对互信息的对称性，统计特性，和缺乏对共同历史和输入信号的描述



考虑一个复合系统，我们想要知道这个系统的信息结构，那么我们应该从哪些方面着手？

第一，应该是子系统对总体的信息贡献，也就是说子系统以何种速率产生信息

第二，应该是子系统之间的信息交换速率。在这篇文章里面，我们关注的是第二个问题

并且，我们关注的对象并不是一个静态的系统，这种系统没什么值得研究的，我们关注的是时间演化系统，简而言之就是一个时间序列





首先，我们要回顾一下基本的概念：

1.  香农信息熵，意义，底数，基数，量纲，比特，奈特

    熵的数学含义，自指涉数学期望

    ​

2.  针对条件概率也有条件熵的定义,针对联合概率也有联合熵，这些都很自然，联合熵意味着系统的熵和

    ​

3.  我们可以引入互信息的概念，互信息就是当给定另一个随机变量的信息的时候，导致随机变量的熵的减少量，或者说不确定度的减少量，也可以说是在两个随机变量有关系的情况下，错误的认为他们无关系，两者的熵的差

接下来，我们需要引入另外一个比较重要的概念，相对熵

4.  如果我们要计算香农熵，那么我们需要知道分布的情况，但是如果我们并不清楚，而是才用了一个错误的猜想，那么这将导致我们会需要使用更多的信息位去描述一个变量·，这时我们需要的信息位数称之交叉熵，注意这个交叉熵的表达式，我们如果从数学期望的角度来看就比较好理解了

5.  那么我们需要多多少呢，二者之差称为相对熵

6.  接下来考虑两个再稍微复杂一点的概念，条件相对熵，联合相对熵

    条件相对熵的意义是，假设xy的条件概率符合p分布，但是我们却错误的认为他是q分布，那么得到的相对熵应该是：

    联合相对熵，考虑一下之前说的互信息的含义，再考虑一下联合相对熵的含义，你会发现两者应该是一样的，

    数学也证明的确如此



7.  此时回过头来，思考一下我们想要拿这些东西来做些什么。我们试图衡量两个系统之间的信息交换，交换量和方向都是十分重要的，这是我们就要观察一下我们已经拿到的量里面那些具有这个作用

其实最后一个时延互信息给了我们一些启发，我们应该去包含了时间的系统里面去寻找解决方案，恰好，正如我们一开始所说的，我们关注的系统也正是时间演化系统，从静态的概率到与时间相关的序列，我们自然而然就想到了我们很熟悉的一个概念：随机过程



考虑随机过程，将静态的概率转换为转移概率



8.  随机过程的概率与时间无关称为平稳的随机过程
9.  如果随机序列中的 每个随机变量值依赖于前几个状态，那么称之为马尔科夫过程，特殊的还有k阶马尔科夫过程

我们考虑平稳的k阶马尔科夫过程

考虑熵率的概念，对于随机过程来说随机变量一般不独立，当时平稳过程的时候，系统的熵会随n的增加以一定的速率线性增长，我们成这个速率为熵率

此时，我们可以考虑对于k阶马尔科夫过程的条件相对熵，

假设有两个系统，其中的I系统是一个k阶马尔科夫过程

所以



对于动态的系统，正如前面所说，香农熵是随n不断增加的，因而在动态系统中，我们倾向于使用熵率来替代熵的位置

并且对于概率而言，明显，联合概率是对称的，不对称的是条件概率，因而我们

最终得到了转移熵



在实际应用过程中，k和l的取值经常是从计算的角度上取得，例如k=l=1

接下来，论文中给出了几个计算的例子，

在离散的情况下，计算是比较简单的，因为这些概率都很容易得到

