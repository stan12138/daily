##讲稿

我读的这篇论文的题目叫做测量信息的传递

首先来看一下这篇论文在做一件什么事情。

在统计学中，我们使用概率来表征事件的可能性，使用条件概率，联合概率等来衡量不同随机变量之间的关系。在信息论中，我们使用香农熵来衡量一个事件或者系统所包含的信息量得多少，或者是不确定度的大小，使用互信息来在一定程度上表征两个系统之间的不确定度或者信息量的关系

互信息是对称的，它不包含任何方向信息。也提出过使用包含了延迟因子的互信息来让它携带一定的方向信息，但是这个并不能揭示任何底层原理，而更类似于一种技术手段。

于是这里提出了一个新的量--转移熵。转移熵可以用来定量的描述两个系统之间的因果性，或者说系统之间的信息流向和大小。

这个量的目标就是对互信息做一些改进，提升他在衡量信息转移上的表现。主要针对互信息的对称性，统计特性，和缺乏对共同历史和输入信号的描述



首先，我们要回顾一下基本的概念：

香农信息熵，意义，底数

然后，针对条件概率也有条件熵的定义,针对联合概率也有联合熵，这些都很自然

这时，我们可以引入互信息的概念，互信息就是当给定另一个随机变量的信息的时候，导致随机变量的熵的减少量，或者说不确定度的减少量，也可以说是在两个随机变量有关系的情况下，错误的认为他们无关系，两者的熵的差

接下来，我们需要引入另外一个比较重要的概念，相对熵

如果我们要计算香农熵，那么我们需要知道分布的情况，但是如果我们并不清楚，而是才用了一个错误的猜想，那么这将导致我们会需要使用更多的信息位去描述一个变量·，这时我们需要的信息位数称之交叉熵

那么我们需要多多少呢，二者之差称为相对熵

接下来考虑两个再稍微复杂一点的概念，条件相对熵，联合相对熵

条件相对熵的意义是，假设xy的条件概率符合p分布，但是我们却错误的认为他是q分布，那么得到的相对熵应该是：

联合相对熵

对称与否



考虑随机过程，将静态的概率转换为转移概率

随机过程的概率与时间无关称为平稳的随机过程

如果随机序列中的每个随机变量值依赖于前几个状态，那么称之为马尔科夫过程，特殊的还有k阶马尔科夫过程

我们考虑平稳的k阶马尔科夫过程

考虑熵率的概念，对于随机过程来说随机变量一般不独立，当时平稳过程的时候，系统的熵会随n的增加以一定的速率线性增长，我们成这个速率为熵率

此时，我们可以考虑对于k阶马尔科夫过程的条件相对熵，

假设有两个系统，其中的I系统是一个k阶马尔科夫过程

所以



最终得到了转移熵



在实际应用过程中，k和l的取值经常是从计算的角度上取得，例如k=l=1

接下来，论文中给出了几个计算的例子，

在离散的情况下，计算是比较简单的，因为这些概率都很容易得到

